# -*- coding: utf-8 -*-
"""Assignment6_10170832.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PEhkLcMb1eiNrRa9-PASKAa8_sKdESN3
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np 
import tensorflow as tf
# %matplotlib inline
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import cross_validate
from sklearn.model_selection import GridSearchCV
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

#Load in the Cleveland data, and set up as a PD data frame
df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/processed.cleveland.data', header = None, na_values = "?")
df.head()

#Load all of the data into a dataframe, and give it labels 
df.columns = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','num'] 

#Split the data into two subsets, X= The varaibles of the data, y = that target of the data
X = df[['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal']]
y = df['num']

#Using pipeline, we an fit Scale, Encode, and impute our X dataset at once. This will create a new
#dataframe that will be used as the new X vairable 
categorical = [2, 6, 10, 12]
continuous = [0, 3, 4, 7, 9, 11]

col_tf = ColumnTransformer(
    [('scale', preprocessing.StandardScaler(), continuous),
     ('onehot', preprocessing.OneHotEncoder(categories="auto"), categorical)],
     remainder='passthrough')
 
pipe = Pipeline(
    [('impute', SimpleImputer(missing_values=np.nan, strategy='mean')),
     ('scale_onehot', col_tf)])

X_Final = pd.DataFrame(pipe.fit_transform(X))
X_Final.head()

#OneHotEncode the y variable using karis 
y_final = tf.keras.utils.to_categorical(y)

#Now we can split our data into training and test sets at a 70/30% ratio 
X_train, X_test, y_train, y_test = train_test_split(X_Final, y_final, test_size=0.3, random_state=15)

nlist = []
Llist = []

for n in np.arange (5,51,5): 
  nlist.append(n)
  model = Sequential([
              Dense(n, activation = 'relu'),
              Dense(5, activation = 'softmax')
  ])
  model.compile(optimizer = 'adam',
                loss = 'categorical_crossentropy',
                metrics = ['accuracy'])
  
  history = model.fit(X_train, y_train, epochs = 25, validation_split = 0.2, batch_size = 32)
  tempLoss =  history.history['val_loss']
  Llist.append(tempLoss)

print("best n: ", nlist[Llist.index(max(Llist))])
#print("loss: ", Llist[Llist.index(max(Llist))])

modelF = Sequential([
              Dense(nlist[Llist.index(max(Llist))], activation = 'relu'),
              Dense(5, activation = 'softmax')
])
modelF.compile(optimizer = 'adam',
                loss = 'categorical_crossentropy',
                metrics = ['accuracy'])
modelF.fit(X_train, y_train, epochs = 25)

#Evaluate the final model on the test data 
valLoss, accuracy = modelF.evaluate(X_test,  y_test)
print("value loss of final model: {}\nAccuracy of final model: {} ".format(valLoss, accuracy))